"""
åŠ¨æ€Optimizeræ³¨å†Œè¡¨ - QuantumForge vNext

åŸºäºæ³¨å†Œè¡¨é©±åŠ¨çš„optimizerç®¡ç†ç³»ç»Ÿï¼Œæ”¯æŒåŠ¨æ€æ‰©å±•å’Œè‡ªåŠ¨ç»„ä»¶ç”Ÿæˆã€‚
æ›¿ä»£ç¡¬ç¼–ç çš„optimizeråˆ—è¡¨å’Œé‡å¤çš„helperå‡½æ•°ã€‚
"""

from typing import Dict, Any, List, Optional, Union
import importlib
import inspect
# ComponentCardç›´æ¥ä½¿ç”¨Dictæ›¿ä»£


# =============================================================================
# Optimizeræ³¨å†Œè¡¨ - æ ¸å¿ƒé…ç½®
# =============================================================================

OPTIMIZER_REGISTRY = {
    "COBYLA": {
        "module": "qiskit_algorithms.optimizers",
        "class_name": "COBYLA", 
        "tags": ["gradient_free", "classical", "derivative_free"],
        "description": "Constrained Optimization BY Linear Approximation",
        "params": {
            "maxiter": {"type": "int", "default": 1000, "description": "Maximum iterations"},
            "rhobeg": {"type": "float", "default": 1.0, "description": "Initial trust region radius"},
            "rhoend": {"type": "float", "default": 1e-4, "description": "Final trust region radius"},
            "disp": {"type": "bool", "default": False, "description": "Display convergence info"}
        },
        "supports_bounds": False,
        "supports_constraints": True
    },
    
    "L_BFGS_B": {
        "module": "qiskit_algorithms.optimizers",
        "class_name": "L_BFGS_B",
        "tags": ["gradient_based", "classical", "quasi_newton"], 
        "description": "L-BFGS-B optimizer for bounded optimization",
        "params": {
            "maxiter": {"type": "int", "default": 1000, "description": "Maximum iterations"},
            "ftol": {"type": "float", "default": 1e-9, "description": "Function tolerance"},
            "gtol": {"type": "float", "default": 1e-5, "description": "Gradient tolerance"},
            "eps": {"type": "float", "default": 1e-8, "description": "Step size for gradient approximation"}
        },
        "supports_bounds": True,
        "supports_constraints": False
    },
    
    "SPSA": {
        "module": "qiskit_algorithms.optimizers",
        "class_name": "SPSA",
        "tags": ["stochastic", "gradient_free", "simultaneous_perturbation"],
        "description": "Simultaneous Perturbation Stochastic Approximation",
        "params": {
            "maxiter": {"type": "int", "default": 1000, "description": "Maximum iterations"},
            "learning_rate": {"type": "float", "default": None, "description": "Learning rate"},
            "perturbation": {"type": "float", "default": None, "description": "Perturbation factor"},
            "resamplings": {"type": "int", "default": 1, "description": "Number of resamplings"}
        },
        "supports_bounds": False,
        "supports_constraints": False
    },
    
    "SLSQP": {
        "module": "qiskit_algorithms.optimizers", 
        "class_name": "SLSQP",
        "tags": ["gradient_based", "classical", "sequential"],
        "description": "Sequential Least SQuares Programming",
        "params": {
            "maxiter": {"type": "int", "default": 100, "description": "Maximum iterations"},
            "ftol": {"type": "float", "default": 1e-9, "description": "Function tolerance"},
            "eps": {"type": "float", "default": 1.49e-8, "description": "Step size for gradient approximation"}
        },
        "supports_bounds": True,
        "supports_constraints": True
    },
    
    "POWELL": {
        "module": "qiskit_algorithms.optimizers",
        "class_name": "POWELL", 
        "tags": ["gradient_free", "classical", "powell_method"],
        "description": "Powell's conjugate direction method",
        "params": {
            "maxiter": {"type": "int", "default": 1000, "description": "Maximum iterations"},
            "maxfev": {"type": "int", "default": None, "description": "Maximum function evaluations"},
            "xtol": {"type": "float", "default": 1e-4, "description": "Relative tolerance"},
            "ftol": {"type": "float", "default": 1e-4, "description": "Relative function tolerance"}
        },
        "supports_bounds": True,
        "supports_constraints": False
    },
    
    "TNC": {
        "module": "qiskit_algorithms.optimizers",
        "class_name": "TNC",
        "tags": ["gradient_based", "classical", "truncated_newton"],
        "description": "Truncated Newton method for bounded optimization",
        "params": {
            "maxiter": {"type": "int", "default": 100, "description": "Maximum iterations"},
            "ftol": {"type": "float", "default": -1, "description": "Function tolerance"},
            "xtol": {"type": "float", "default": -1, "description": "Parameter tolerance"},
            "gtol": {"type": "float", "default": -1, "description": "Gradient tolerance"}
        },
        "supports_bounds": True,
        "supports_constraints": False
    }
}


# =============================================================================
# åŠ¨æ€ç»„ä»¶ç”Ÿæˆå™¨
# =============================================================================

def generate_optimizer_components() -> List[Dict[str, Any]]:
    """
    åŸºäºæ³¨å†Œè¡¨åŠ¨æ€ç”Ÿæˆæ‰€æœ‰optimizerç»„ä»¶å®šä¹‰
    
    Returns:
        ç»„ä»¶å­—å…¸åˆ—è¡¨ï¼Œå¯ç›´æ¥ç”¨äºç»„ä»¶å‘ç°ç³»ç»Ÿ
    """
    components = []
    
    for optimizer_name, config in OPTIMIZER_REGISTRY.items():
        component = {
            "name": f"Optimizer.{optimizer_name}",
            "kind": "optimizer",
            "tags": config["tags"] + ["optimizer"],
            "needs": [],
            "provides": ["optimizer"],
            "params_schema": _build_params_schema(config["params"]),
            "yields": {
                "optimizer": f"{optimizer_name} optimizer instance"
            },
            "codegen_hint": {
                "helper": f"create_{optimizer_name.lower()}_optimizer",
                "import": f"from {config['module']} import {config['class_name']}"
            },
            "metadata": {
                "description": config["description"],
                "supports_bounds": config["supports_bounds"],
                "supports_constraints": config["supports_constraints"]
            }
        }
        components.append(component)
    
    return components


def _build_params_schema(params_config: Dict[str, Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
    """æ„å»ºå‚æ•°Schema"""
    schema = {}
    for param_name, param_info in params_config.items():
        schema[param_name] = {
            "type": param_info["type"],
            "description": param_info["description"]
        }
        if param_info.get("default") is not None:
            schema[param_name]["default"] = param_info["default"]
    return schema


# =============================================================================
# ç»Ÿä¸€Optimizerå·¥å‚
# =============================================================================

def create_optimizer_factory(optimizer_name: str, **kwargs) -> Any:
    """
    ç»Ÿä¸€çš„optimizeråˆ›å»ºå·¥å‚
    
    Args:
        optimizer_name: optimizeråç§° (å¦‚ "COBYLA", "L_BFGS_B")
        **kwargs: optimizerå‚æ•°
        
    Returns:
        é…ç½®å¥½çš„optimizerå®ä¾‹
        
    Raises:
        ValueError: ä¸æ”¯æŒçš„optimizerç±»å‹
        ImportError: optimizerç±»å¯¼å…¥å¤±è´¥
    """
    if optimizer_name not in OPTIMIZER_REGISTRY:
        raise ValueError(f"ä¸æ”¯æŒçš„optimizer: {optimizer_name}")
    
    config = OPTIMIZER_REGISTRY[optimizer_name]
    
    try:
        # åŠ¨æ€å¯¼å…¥optimizerç±»
        module = importlib.import_module(config["module"])
        optimizer_class = getattr(module, config["class_name"])
        
        # åº”ç”¨é»˜è®¤å‚æ•°
        final_params = {}
        for param_name, param_config in config["params"].items():
            if param_name in kwargs:
                final_params[param_name] = kwargs[param_name]
            elif param_config.get("default") is not None:
                final_params[param_name] = param_config["default"]
        
        # åˆ›å»ºoptimizerå®ä¾‹
        return optimizer_class(**final_params)
        
    except ImportError as e:
        raise ImportError(f"æ— æ³•å¯¼å…¥optimizer {optimizer_name}: {e}")
    except Exception as e:
        raise RuntimeError(f"åˆ›å»ºoptimizer {optimizer_name}å¤±è´¥: {e}")


def get_available_optimizers() -> List[str]:
    """è·å–æ‰€æœ‰å¯ç”¨çš„optimizeråç§°"""
    return list(OPTIMIZER_REGISTRY.keys())


def get_optimizer_info(optimizer_name: str) -> Optional[Dict[str, Any]]:
    """è·å–ç‰¹å®šoptimizerçš„è¯¦ç»†ä¿¡æ¯"""
    return OPTIMIZER_REGISTRY.get(optimizer_name)


def get_optimizers_by_tag(tag: str) -> List[str]:
    """æ ¹æ®æ ‡ç­¾è·å–optimizeråˆ—è¡¨"""
    result = []
    for name, config in OPTIMIZER_REGISTRY.items():
        if tag in config["tags"]:
            result.append(name)
    return result


def register_optimizer(name: str, config: Dict[str, Any]) -> None:
    """
    æ³¨å†Œæ–°çš„optimizer
    
    Args:
        name: optimizeråç§°
        config: optimizeré…ç½® (æ ¼å¼åŒOPTIMIZER_REGISTRY)
    """
    OPTIMIZER_REGISTRY[name] = config


def validate_optimizer_availability() -> Dict[str, bool]:
    """
    éªŒè¯æ‰€æœ‰æ³¨å†Œçš„optimizeræ˜¯å¦å¯ç”¨
    
    Returns:
        {optimizer_name: is_available}
    """
    availability = {}
    
    for optimizer_name, config in OPTIMIZER_REGISTRY.items():
        try:
            module = importlib.import_module(config["module"])
            optimizer_class = getattr(module, config["class_name"])
            availability[optimizer_name] = True
        except (ImportError, AttributeError):
            availability[optimizer_name] = False
    
    return availability


# =============================================================================
# Helperå‡½æ•°åŠ¨æ€ç”Ÿæˆå™¨
# =============================================================================

def generate_optimizer_helper_code() -> str:
    """
    ç”Ÿæˆæ‰€æœ‰optimizerçš„helperå‡½æ•°ä»£ç 
    
    Returns:
        å®Œæ•´çš„helperå‡½æ•°Pythonä»£ç 
    """
    import_lines = []
    function_lines = []
    
    for optimizer_name, config in OPTIMIZER_REGISTRY.items():
        # ç”Ÿæˆimport
        import_line = f"from {config['module']} import {config['class_name']}"
        if import_line not in import_lines:
            import_lines.append(import_line)
        
        # ç”Ÿæˆhelperå‡½æ•°
        func_name = f"create_{optimizer_name.lower()}_optimizer"
        params = config["params"]
        
        # æ„å»ºå‡½æ•°ç­¾å
        param_list = []
        for param_name, param_config in params.items():
            default = param_config.get("default")
            if default is not None:
                if isinstance(default, str):
                    param_list.append(f'{param_name}="{default}"')
                else:
                    param_list.append(f'{param_name}={default}')
            else:
                param_list.append(f'{param_name}=None')
        
        param_signature = ", ".join(param_list)
        
        # æ„å»ºå‡½æ•°å‚æ•°è¿‡æ»¤
        filtered_params = []
        for param_name in params.keys():
            filtered_params.append(f'        if {param_name} is not None: kwargs["{param_name}"] = {param_name}')
        
        function_code = f'''def {func_name}({param_signature}):
    """{config["description"]}"""
    kwargs = {{}}
{chr(10).join(filtered_params)}
    return {config["class_name"]}(**kwargs)'''
        
        function_lines.append(function_code)
    
    # ç»„åˆå®Œæ•´ä»£ç 
    full_code = f'''"""
åŠ¨æ€ç”Ÿæˆçš„Optimizer Helperå‡½æ•°
è‡ªåŠ¨ä»optimizer_registry.pyç”Ÿæˆï¼Œè¯·å‹¿æ‰‹åŠ¨ç¼–è¾‘
"""

{chr(10).join(import_lines)}


{chr(10).join(function_lines)}
'''
    
    return full_code


# =============================================================================
# æµ‹è¯•å’ŒéªŒè¯å‡½æ•°
# =============================================================================

if __name__ == "__main__":
    print("ğŸ§ª æµ‹è¯•Optimizeræ³¨å†Œè¡¨...")
    
    # 1. æµ‹è¯•å¯ç”¨æ€§
    print("\\nğŸ“‹ å¯ç”¨Optimizer:")
    availability = validate_optimizer_availability()
    for name, available in availability.items():
        status = "âœ…" if available else "âŒ"
        print(f"  {status} {name}")
    
    # 2. æµ‹è¯•å·¥å‚å‡½æ•°
    print("\\nğŸ­ æµ‹è¯•å·¥å‚åˆ›å»º:")
    try:
        cobyla = create_optimizer_factory("COBYLA", maxiter=500)
        print(f"  âœ… COBYLAåˆ›å»ºæˆåŠŸ: {type(cobyla).__name__}")
        
        lbfgs = create_optimizer_factory("L_BFGS_B", maxiter=200, ftol=1e-8)
        print(f"  âœ… L_BFGS_Båˆ›å»ºæˆåŠŸ: {type(lbfgs).__name__}")
    except Exception as e:
        print(f"  âŒ åˆ›å»ºå¤±è´¥: {e}")
    
    # 3. æµ‹è¯•æ ‡ç­¾æŸ¥è¯¢
    print("\\nğŸ·ï¸ æŒ‰æ ‡ç­¾æŸ¥è¯¢:")
    gradient_free = get_optimizers_by_tag("gradient_free")
    print(f"  gradient_free: {gradient_free}")
    
    # 4. æµ‹è¯•ç»„ä»¶ç”Ÿæˆ
    print("\\nğŸ§± æµ‹è¯•ç»„ä»¶ç”Ÿæˆ:")
    components = generate_optimizer_components()
    print(f"  ç”Ÿæˆäº†{len(components)}ä¸ªoptimizerç»„ä»¶")
    for comp in components[:2]:
        print(f"    - {comp['name']}: {comp['metadata']['description']}")
    
    # 5. ç”Ÿæˆhelperä»£ç ç¤ºä¾‹
    print("\\nğŸ“ Helperå‡½æ•°ä»£ç é¢„è§ˆ:")
    helper_code = generate_optimizer_helper_code()
    preview = helper_code.split('\n')[:15]
    for line in preview:
        print(f"    {line}")
    print("    ...")
    
    print("\\nğŸ‰ æ³¨å†Œè¡¨æµ‹è¯•å®Œæˆ!")